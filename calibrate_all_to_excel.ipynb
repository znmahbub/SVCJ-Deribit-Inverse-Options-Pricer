{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4536cd67",
      "metadata": {},
      "source": [
        "# Calibrate all snapshots (BTC and ETH) → Excel workbook\n",
        "\n",
        "This notebook calibrates **Black**, **Heston**, and **SVCJ** to **all** `deribit_options_snapshot_*.csv` files in `data/`, then persists:\n",
        "\n",
        "- model parameters (3 sheets)\n",
        "- train and test datasets used (2 sheets), each with **three additional columns**: `price_black`, `price_heston`, `price_svcj`\n",
        "\n",
        "Key features:\n",
        "\n",
        "- **Resume**: if the Excel file already exists, we continue from the first snapshot **after** the latest `timestamp` present in `black_params` for each currency.\n",
        "- **Periodic saving**: flush to Excel every `SAVE_EVERY_N_FILES` processed snapshots (atomic write).\n",
        "- **Warm start**: per-thread warm start within chunk; chunk-start warm start pulled from the existing workbook (latest successful params prior to the chunk's first timestamp).\n",
        "- **Threading**: multiple worker threads process contiguous chunks; the main thread commits results **in chronological order** to keep resume safe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4770f5e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and project path setup\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import math\n",
        "import contextlib\n",
        "import threading\n",
        "import queue\n",
        "from pathlib import Path\n",
        "from tempfile import NamedTemporaryFile\n",
        "from typing import Optional, Dict, Tuple, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    from threadpoolctl import threadpool_limits\n",
        "except Exception:\n",
        "    threadpool_limits = None\n",
        "\n",
        "# --- locate project root (folder that contains `src/` and `data/`)\n",
        "ROOT = Path.cwd().resolve()\n",
        "candidates = [ROOT, ROOT.parent, *ROOT.parents]\n",
        "for c in candidates:\n",
        "    if (c / \"src\").exists() and (c / \"data\").exists():\n",
        "        ROOT = c\n",
        "        break\n",
        "\n",
        "assert (ROOT / \"src\").exists() and (ROOT / \"data\").exists(), f\"Could not find project root from cwd={Path.cwd()}\"\n",
        "\n",
        "# Make project importable\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT))\n",
        "\n",
        "from src.calibration import (\n",
        "    filter_liquid_options,\n",
        "    calibrate_model,\n",
        "    price_dataframe,\n",
        "    WeightConfig,\n",
        ")\n",
        "from src.inverse_fft_pricer import FFTParams\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4668360",
      "metadata": {},
      "source": [
        "## Global configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74fbfd74",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Output / resume\n",
        "# -----------------------------\n",
        "OUTPUT_XLSX = ROOT / \"calibration_results.xlsx\"\n",
        "RESUME = True\n",
        "SAVE_EVERY_N_FILES = 10  # flush workbook every N committed snapshots (per currency run)\n",
        "\n",
        "# If you want a quick smoke test, set to an integer (e.g., 1 or 2). None means process all.\n",
        "SMOKE_TEST_MAX_FILES_PER_CURRENCY: Optional[int] = None\n",
        "\n",
        "# -----------------------------\n",
        "# Global filter rules (liquidity cleaning)\n",
        "# -----------------------------\n",
        "FILTER_RULES = dict(\n",
        "    require_bid_ask=True,             # drop rows missing bid OR ask (hard requirement)\n",
        "    min_time_to_maturity=1/365,\n",
        "    max_time_to_maturity=None,\n",
        "    min_open_interest=1.0,\n",
        "    min_vega=0.0,\n",
        "    max_rel_spread=0.50,              # relative spread cap (ask-bid)/mid\n",
        "    moneyness_range=(0.5, 2.0),       # K/F0 range\n",
        "    drop_synthetic_underlyings=False, # optionally drop SYN.* futures underlyings\n",
        ")\n",
        "\n",
        "# Skip snapshot/currency if too few options remain after filtering\n",
        "MIN_OPTIONS_AFTER_FILTER = 50\n",
        "\n",
        "# -----------------------------\n",
        "# Parallelism (per-snapshot processing)\n",
        "# -----------------------------\n",
        "# N_THREADS = max(1, int((os.cpu_count() or 4) - 2))\n",
        "N_THREADS = 6\n",
        "\n",
        "# Prevent oversubscription: each worker may call heavy NumPy/SciPy/FFT kernels.\n",
        "LIMIT_INTERNAL_THREADS = True\n",
        "INTERNAL_NUM_THREADS = 1  # recommended = 1 when using N_THREADS > 1\n",
        "\n",
        "# -----------------------------\n",
        "# Global weight config (used in calibration residuals)\n",
        "# r_i = w_i * (P_model_i - P_mkt_i)\n",
        "# -----------------------------\n",
        "WEIGHT_CONFIG = WeightConfig(\n",
        "    use_spread=True,\n",
        "    use_vega=False,\n",
        "    use_open_interest=False,\n",
        "    spread_power=1.0,\n",
        "    vega_power=0.5,\n",
        "    oi_power=0.5,\n",
        "    eps_spread=1e-6,\n",
        "    eps_other=1e-12,\n",
        "    cap=1e6,\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Carr–Madan FFT base parameters\n",
        "# (dynamic_b=False here because we precompute per-expiry b per snapshot)\n",
        "# -----------------------------\n",
        "FFT_BASE = FFTParams(\n",
        "    N=2**12,\n",
        "    eta=0.10,\n",
        "    alpha=1.5,\n",
        "    b=-10.0,          # overridden by per-expiry b\n",
        "    use_simpson=True,\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Calibration knobs\n",
        "# -----------------------------\n",
        "TRAIN_FRAC = 0.70\n",
        "GLOBAL_RANDOM_SEED = 123\n",
        "\n",
        "# max_nfev is the main runtime control (per model per snapshot)\n",
        "MAX_NFEV = dict(\n",
        "    black=200,\n",
        "    heston=200,\n",
        "    svcj=200,\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Runtime guards (optional, recommended when looping many snapshots)\n",
        "# -----------------------------\n",
        "RUNTIME_TOP_EXPIRIES_BY_OI = None   # keep only top expiries by total OI per snapshot/currency\n",
        "RUNTIME_MAX_OPTIONS = None          # cap number of options per snapshot/currency after filtering\n",
        "\n",
        "# -----------------------------\n",
        "# Currency list\n",
        "# -----------------------------\n",
        "CURRENCIES = [\"BTC\", \"ETH\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf65e49b",
      "metadata": {},
      "source": [
        "## Excel schema + persistence helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93bba16f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Excel schema and helpers\n",
        "PARAM_SHEET_BLACK = \"black_params\"\n",
        "PARAM_SHEET_HESTON = \"heston_params\"\n",
        "PARAM_SHEET_SVCJ = \"svcj_params\"\n",
        "TRAIN_SHEET = \"train_data\"\n",
        "TEST_SHEET = \"test_data\"\n",
        "\n",
        "PARAM_COLS_COMMON = [\n",
        "    \"timestamp\",          # snapshot timestamp from filename (ISO string, e.g. 2026-02-04T09:36:20Z)\n",
        "    \"currency\",\n",
        "    \"success\",\n",
        "    \"message\",\n",
        "    \"nfev\",\n",
        "    \"rmse_fit\",\n",
        "    \"mae_fit\",\n",
        "    \"rmse_train\",\n",
        "    \"mae_train\",\n",
        "    \"rmse_test\",\n",
        "    \"mae_test\",\n",
        "    \"n_options_total\",\n",
        "    \"n_train\",\n",
        "    \"n_test\",\n",
        "    \"random_seed\",\n",
        "]\n",
        "\n",
        "PARAM_COLS_BLACK = PARAM_COLS_COMMON + [\"sigma\"]\n",
        "PARAM_COLS_HESTON = PARAM_COLS_COMMON + [\"kappa\", \"theta\", \"sigma_v\", \"rho\", \"v0\"]\n",
        "PARAM_COLS_SVCJ = PARAM_COLS_COMMON + [\"kappa\", \"theta\", \"sigma_v\", \"rho\", \"v0\", \"lam\", \"ell_y\", \"sigma_y\", \"ell_v\", \"rho_j\"]\n",
        "\n",
        "# Preferred (front) columns for train/test sheets; any other columns will be appended after these.\n",
        "TRAIN_TEST_FRONT_COLS = [\n",
        "    \"snapshot_ts\",\n",
        "    \"currency\",\n",
        "    \"instrument_name\",\n",
        "    \"option_type\",\n",
        "    \"strike\",\n",
        "    \"expiry_datetime\",\n",
        "    \"time_to_maturity\",\n",
        "    \"futures_price\",\n",
        "    \"bid_price\",\n",
        "    \"ask_price\",\n",
        "    \"mid_price_clean\",\n",
        "    \"rel_spread\",\n",
        "    \"open_interest\",\n",
        "    \"vega\",\n",
        "    \"random_seed\",\n",
        "    \"price_black\",\n",
        "    \"price_heston\",\n",
        "    \"price_svcj\",\n",
        "]\n",
        "\n",
        "def _empty_df(cols: list[str]) -> pd.DataFrame:\n",
        "    return pd.DataFrame({c: pd.Series(dtype=\"object\") for c in cols})\n",
        "\n",
        "def init_empty_workbook() -> dict[str, pd.DataFrame]:\n",
        "    return {\n",
        "        PARAM_SHEET_BLACK: _empty_df(PARAM_COLS_BLACK),\n",
        "        PARAM_SHEET_HESTON: _empty_df(PARAM_COLS_HESTON),\n",
        "        PARAM_SHEET_SVCJ: _empty_df(PARAM_COLS_SVCJ),\n",
        "        TRAIN_SHEET: pd.DataFrame(),\n",
        "        TEST_SHEET: pd.DataFrame(),\n",
        "    }\n",
        "\n",
        "def normalize_timestamp_series(s: pd.Series) -> pd.Series:\n",
        "    \"\"\"Normalize timestamps to ISO strings (UTC with trailing 'Z').\"\"\"\n",
        "    dt = pd.to_datetime(s, utc=True, errors=\"coerce\")\n",
        "    out = dt.dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "    out = out.where(out.notna(), s.astype(str))\n",
        "    return out\n",
        "\n",
        "def ensure_param_columns(df: pd.DataFrame, expected_cols: list[str]) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    for c in expected_cols:\n",
        "        if c not in out.columns:\n",
        "            out[c] = np.nan\n",
        "    out = out[expected_cols]\n",
        "    if \"timestamp\" in out.columns:\n",
        "        out[\"timestamp\"] = normalize_timestamp_series(out[\"timestamp\"])\n",
        "    return out\n",
        "\n",
        "def order_train_test_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Put important columns first; keep the rest in stable (sorted) order.\"\"\"\n",
        "    if df is None or len(df) == 0:\n",
        "        return df if df is not None else pd.DataFrame()\n",
        "    front = [c for c in TRAIN_TEST_FRONT_COLS if c in df.columns]\n",
        "    rest = [c for c in df.columns if c not in front]\n",
        "    rest_sorted = sorted(rest)\n",
        "    return df[front + rest_sorted]\n",
        "\n",
        "def load_existing_workbook(path: Path) -> dict[str, pd.DataFrame]:\n",
        "    if not path.exists():\n",
        "        return init_empty_workbook()\n",
        "\n",
        "    sheets = pd.read_excel(path, sheet_name=None, engine=\"openpyxl\")\n",
        "    wb = init_empty_workbook()\n",
        "\n",
        "    # params sheets: enforce schema\n",
        "    if PARAM_SHEET_BLACK in sheets:\n",
        "        wb[PARAM_SHEET_BLACK] = ensure_param_columns(sheets[PARAM_SHEET_BLACK], PARAM_COLS_BLACK)\n",
        "    if PARAM_SHEET_HESTON in sheets:\n",
        "        wb[PARAM_SHEET_HESTON] = ensure_param_columns(sheets[PARAM_SHEET_HESTON], PARAM_COLS_HESTON)\n",
        "    if PARAM_SHEET_SVCJ in sheets:\n",
        "        wb[PARAM_SHEET_SVCJ] = ensure_param_columns(sheets[PARAM_SHEET_SVCJ], PARAM_COLS_SVCJ)\n",
        "\n",
        "    # train/test: keep as-is; we'll align columns on append\n",
        "    if TRAIN_SHEET in sheets:\n",
        "        wb[TRAIN_SHEET] = sheets[TRAIN_SHEET].copy()\n",
        "        if \"snapshot_ts\" in wb[TRAIN_SHEET].columns:\n",
        "            wb[TRAIN_SHEET][\"snapshot_ts\"] = normalize_timestamp_series(wb[TRAIN_SHEET][\"snapshot_ts\"])\n",
        "    if TEST_SHEET in sheets:\n",
        "        wb[TEST_SHEET] = sheets[TEST_SHEET].copy()\n",
        "        if \"snapshot_ts\" in wb[TEST_SHEET].columns:\n",
        "            wb[TEST_SHEET][\"snapshot_ts\"] = normalize_timestamp_series(wb[TEST_SHEET][\"snapshot_ts\"])\n",
        "\n",
        "    return wb\n",
        "\n",
        "def get_latest_processed_timestamp(wb: dict[str, pd.DataFrame], currency: str) -> Optional[pd.Timestamp]:\n",
        "    \"\"\"Resume key: latest timestamp in black_params for the given currency.\"\"\"\n",
        "    df = wb.get(PARAM_SHEET_BLACK, pd.DataFrame())\n",
        "    if df is None or df.empty or \"timestamp\" not in df.columns:\n",
        "        return None\n",
        "    sub = df[df[\"currency\"].astype(str) == str(currency)].copy()\n",
        "    if sub.empty:\n",
        "        return None\n",
        "    dt = pd.to_datetime(sub[\"timestamp\"], utc=True, errors=\"coerce\")\n",
        "    dt = dt.dropna()\n",
        "    if dt.empty:\n",
        "        return None\n",
        "    return dt.max()\n",
        "\n",
        "def flush_workbook_atomic(wb: dict[str, pd.DataFrame], output_path: Path) -> None:\n",
        "    \"\"\"Write workbook to disk (atomic replace).\"\"\"\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Ensure stable ordering\n",
        "    wb_to_write = dict(wb)\n",
        "\n",
        "    wb_to_write[PARAM_SHEET_BLACK] = ensure_param_columns(wb_to_write.get(PARAM_SHEET_BLACK, pd.DataFrame()), PARAM_COLS_BLACK)\n",
        "    wb_to_write[PARAM_SHEET_HESTON] = ensure_param_columns(wb_to_write.get(PARAM_SHEET_HESTON, pd.DataFrame()), PARAM_COLS_HESTON)\n",
        "    wb_to_write[PARAM_SHEET_SVCJ] = ensure_param_columns(wb_to_write.get(PARAM_SHEET_SVCJ, pd.DataFrame()), PARAM_COLS_SVCJ)\n",
        "\n",
        "    # sort param sheets\n",
        "    for sh in [PARAM_SHEET_BLACK, PARAM_SHEET_HESTON, PARAM_SHEET_SVCJ]:\n",
        "        df = wb_to_write[sh].copy()\n",
        "        if len(df):\n",
        "            df[\"timestamp_dt\"] = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
        "            df = df.sort_values([\"currency\", \"timestamp_dt\"]).drop(columns=[\"timestamp_dt\"])\n",
        "        wb_to_write[sh] = df\n",
        "\n",
        "    # order train/test columns\n",
        "    wb_to_write[TRAIN_SHEET] = order_train_test_columns(wb_to_write.get(TRAIN_SHEET, pd.DataFrame()))\n",
        "    wb_to_write[TEST_SHEET] = order_train_test_columns(wb_to_write.get(TEST_SHEET, pd.DataFrame()))\n",
        "\n",
        "    # sort train/test rows\n",
        "    for sh in [TRAIN_SHEET, TEST_SHEET]:\n",
        "        df = wb_to_write.get(sh, pd.DataFrame()).copy()\n",
        "        if len(df):\n",
        "            if \"snapshot_ts\" in df.columns:\n",
        "                df[\"_snapshot_dt\"] = pd.to_datetime(df[\"snapshot_ts\"], utc=True, errors=\"coerce\")\n",
        "            else:\n",
        "                df[\"_snapshot_dt\"] = pd.NaT\n",
        "            if \"expiry_datetime\" in df.columns:\n",
        "                df[\"_expiry_dt\"] = pd.to_datetime(df[\"expiry_datetime\"], utc=True, errors=\"coerce\")\n",
        "            else:\n",
        "                df[\"_expiry_dt\"] = pd.NaT\n",
        "            sort_cols = [\"currency\", \"_snapshot_dt\", \"_expiry_dt\"]\n",
        "            if \"strike\" in df.columns:\n",
        "                sort_cols.append(\"strike\")\n",
        "            df = df.sort_values(sort_cols).drop(columns=[c for c in [\"_snapshot_dt\", \"_expiry_dt\"] if c in df.columns])\n",
        "        wb_to_write[sh] = df\n",
        "\n",
        "    with NamedTemporaryFile(\"wb\", suffix=\".xlsx\", delete=False) as tmp:\n",
        "        tmp_path = Path(tmp.name)\n",
        "\n",
        "    try:\n",
        "        with pd.ExcelWriter(tmp_path, engine=\"openpyxl\") as writer:\n",
        "            for sheet_name, df in wb_to_write.items():\n",
        "                if df is None:\n",
        "                    df = pd.DataFrame()\n",
        "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "        os.replace(tmp_path, output_path)\n",
        "    finally:\n",
        "        if tmp_path.exists():\n",
        "            try:\n",
        "                tmp_path.unlink()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "def append_df(wb: dict[str, pd.DataFrame], sheet: str, new: pd.DataFrame) -> None:\n",
        "    if new is None or len(new) == 0:\n",
        "        return\n",
        "    if sheet not in wb or wb[sheet] is None or wb[sheet].empty:\n",
        "        wb[sheet] = new.copy()\n",
        "        return\n",
        "    wb[sheet] = pd.concat([wb[sheet], new], ignore_index=True, sort=False)\n",
        "\n",
        "def latest_successful_params_before(\n",
        "    params_df: pd.DataFrame, *, currency: str, ts0: pd.Timestamp, required_cols: list[str]\n",
        ") -> Optional[dict[str, float]]:\n",
        "    \"\"\"Return latest successful params row before ts0 (converted to dict).\"\"\"\n",
        "    if params_df is None or params_df.empty:\n",
        "        return None\n",
        "    sub = params_df.copy()\n",
        "    sub = sub[sub[\"currency\"].astype(str) == str(currency)]\n",
        "    if sub.empty:\n",
        "        return None\n",
        "    sub_dt = pd.to_datetime(sub[\"timestamp\"], utc=True, errors=\"coerce\")\n",
        "    sub = sub.assign(_ts=sub_dt)\n",
        "    sub = sub[(sub[\"_ts\"].notna()) & (sub[\"_ts\"] < ts0)]\n",
        "    if \"success\" in sub.columns:\n",
        "        sub = sub[sub[\"success\"] == True]  # noqa: E712\n",
        "    if sub.empty:\n",
        "        return None\n",
        "    sub = sub.sort_values(\"_ts\").iloc[-1]\n",
        "    out: dict[str, float] = {}\n",
        "    for c in required_cols:\n",
        "        if c in sub.index and pd.notna(sub[c]):\n",
        "            try:\n",
        "                out[c] = float(sub[c])\n",
        "            except Exception:\n",
        "                pass\n",
        "    return out if out else None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4713ca86",
      "metadata": {},
      "source": [
        "## Helper utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6be0d0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_snapshot_files(data_dir: Path) -> list[Path]:\n",
        "    files = sorted(data_dir.glob(\"deribit_options_snapshot_*.csv\"))\n",
        "    files = [f for f in files if not f.name.startswith(\"._\")]  # drop macOS metadata files\n",
        "    return files\n",
        "\n",
        "_TS_RE = re.compile(r\"deribit_options_snapshot_(\\d{8}T\\d{6})Z\\.csv$\")\n",
        "\n",
        "def timestamp_from_filename(path: Path) -> pd.Timestamp:\n",
        "    m = _TS_RE.search(path.name)\n",
        "    if not m:\n",
        "        raise ValueError(f\"Cannot parse timestamp from filename: {path.name}\")\n",
        "    return pd.to_datetime(m.group(1), format=\"%Y%m%dT%H%M%S\", utc=True)\n",
        "\n",
        "def timestamp_to_iso_z(ts: pd.Timestamp) -> str:\n",
        "    ts = pd.to_datetime(ts, utc=True)\n",
        "    return ts.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "\n",
        "def restrict_for_runtime(\n",
        "    df: pd.DataFrame, *, top_expiries: Optional[int], max_options: Optional[int], random_state: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Optional additional selection to keep per-snapshot runtime under control.\"\"\"\n",
        "    out = df.copy()\n",
        "\n",
        "    if top_expiries is not None and top_expiries > 0:\n",
        "        oi_by_expiry = out.groupby(\"expiry_datetime\")[\"open_interest\"].sum().sort_values(ascending=False)\n",
        "        keep_expiries = oi_by_expiry.head(top_expiries).index\n",
        "        out = out[out[\"expiry_datetime\"].isin(keep_expiries)].copy()\n",
        "\n",
        "    if max_options is not None and len(out) > max_options:\n",
        "        out = out.sample(n=max_options, random_state=random_state).copy()\n",
        "\n",
        "    return out.reset_index(drop=True)\n",
        "\n",
        "def compute_errors(y_true: np.ndarray, y_pred: np.ndarray, *, min_finite_frac: float = 0.8) -> dict[str, float]:\n",
        "    \"\"\"Compute MSE/MAE, guarding against occasional non-finite outputs.\"\"\"\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    finite = np.isfinite(y_pred)\n",
        "    if finite.mean() < min_finite_frac:\n",
        "        return {\"mse\": float(\"nan\"), \"mae\": float(\"nan\")}\n",
        "    yt = y_true[finite]\n",
        "    yp = y_pred[finite]\n",
        "    mse = float(np.mean((yp - yt) ** 2))\n",
        "    mae = float(np.mean(np.abs(yp - yt)))\n",
        "    return {\"mse\": mse, \"mae\": mae}\n",
        "\n",
        "def chunk_contiguous(items: list[Any], n_chunks: int) -> list[list[tuple[int, Any]]]:\n",
        "    \"\"\"Split items into n_chunks contiguous chunks, returning list of (index,item) lists.\"\"\"\n",
        "    n = len(items)\n",
        "    if n == 0:\n",
        "        return []\n",
        "    n_chunks = max(1, min(n_chunks, n))\n",
        "    sizes = [(n // n_chunks) + (1 if i < (n % n_chunks) else 0) for i in range(n_chunks)]\n",
        "    chunks: list[list[tuple[int, Any]]] = []\n",
        "    start = 0\n",
        "    for sz in sizes:\n",
        "        chunk = [(start + j, items[start + j]) for j in range(sz)]\n",
        "        chunks.append(chunk)\n",
        "        start += sz\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9cad49c",
      "metadata": {},
      "source": [
        "## Core routine: process one snapshot and one currency → Excel payload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f90db7a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_snapshot_to_excel_payload(\n",
        "    csv_path: Path,\n",
        "    *,\n",
        "    currency: str,\n",
        "    filter_rules: dict,\n",
        "    weight_config: WeightConfig,\n",
        "    fft_base: FFTParams,\n",
        "    max_nfev: dict,\n",
        "    train_frac: float,\n",
        "    random_seed: int,\n",
        "    runtime_top_expiries_by_oi: Optional[int],\n",
        "    runtime_max_options: Optional[int],\n",
        "    min_options_after_filter: int,\n",
        "    warm_start: Optional[dict[str, dict[str, float]]] = None,\n",
        "    verbose: bool = False,\n",
        ") -> dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Returns a payload dict containing:\n",
        "      - timestamp_iso, currency\n",
        "      - param_rows: {black: df1row, heston: df1row, svcj: df1row}\n",
        "      - train_df, test_df (each includes price_* columns)\n",
        "      - warm_next (dict for warm start)\n",
        "    \"\"\"\n",
        "    ts = timestamp_from_filename(csv_path)\n",
        "    ts_iso = timestamp_to_iso_z(ts)\n",
        "\n",
        "    warm_start = warm_start or {}\n",
        "    warm_next: dict[str, dict[str, float]] = {k: dict(v) for k, v in warm_start.items()}\n",
        "\n",
        "    empty_tt = pd.DataFrame()\n",
        "\n",
        "    def _make_param_row(model: str, *, success: bool, message: str, nfev: int,\n",
        "                        rmse_fit: float, mae_fit: float, rmse_train: float, mae_train: float,\n",
        "                        rmse_test: float, mae_test: float,\n",
        "                        n_total: int, n_train: int, n_test: int,\n",
        "                        params: Optional[dict[str, float]]) -> pd.DataFrame:\n",
        "        base = dict(\n",
        "            timestamp=ts_iso,\n",
        "            currency=currency,\n",
        "            success=bool(success),\n",
        "            message=str(message),\n",
        "            nfev=int(nfev),\n",
        "            rmse_fit=float(rmse_fit) if rmse_fit is not None else float(\"nan\"),\n",
        "            mae_fit=float(mae_fit) if mae_fit is not None else float(\"nan\"),\n",
        "            rmse_train=float(rmse_train) if rmse_train is not None else float(\"nan\"),\n",
        "            mae_train=float(mae_train) if mae_train is not None else float(\"nan\"),\n",
        "            rmse_test=float(rmse_test) if rmse_test is not None else float(\"nan\"),\n",
        "            mae_test=float(mae_test) if mae_test is not None else float(\"nan\"),\n",
        "            n_options_total=int(n_total),\n",
        "            n_train=int(n_train),\n",
        "            n_test=int(n_test),\n",
        "            random_seed=int(random_seed),\n",
        "        )\n",
        "        params = params or {}\n",
        "        base.update(params)\n",
        "        return pd.DataFrame([base])\n",
        "\n",
        "    # --- Load & filter\n",
        "    df_raw = pd.read_csv(csv_path)\n",
        "    df_ccy = df_raw[df_raw[\"currency\"].astype(str) == str(currency)].copy()\n",
        "\n",
        "    if df_ccy.empty:\n",
        "        msg = f\"No rows found for currency={currency} in this snapshot.\"\n",
        "        black_row = _make_param_row(\"black\", success=False, message=msg, nfev=0,\n",
        "                                   rmse_fit=np.nan, mae_fit=np.nan, rmse_train=np.nan, mae_train=np.nan,\n",
        "                                   rmse_test=np.nan, mae_test=np.nan,\n",
        "                                   n_total=0, n_train=0, n_test=0, params={\"sigma\": np.nan})\n",
        "        heston_row = _make_param_row(\"heston\", success=False, message=msg, nfev=0,\n",
        "                                    rmse_fit=np.nan, mae_fit=np.nan, rmse_train=np.nan, mae_train=np.nan,\n",
        "                                    rmse_test=np.nan, mae_test=np.nan,\n",
        "                                    n_total=0, n_train=0, n_test=0, params={})\n",
        "        svcj_row = _make_param_row(\"svcj\", success=False, message=msg, nfev=0,\n",
        "                                  rmse_fit=np.nan, mae_fit=np.nan, rmse_train=np.nan, mae_train=np.nan,\n",
        "                                  rmse_test=np.nan, mae_test=np.nan,\n",
        "                                  n_total=0, n_train=0, n_test=0, params={})\n",
        "        return {\n",
        "            \"timestamp_iso\": ts_iso,\n",
        "            \"currency\": currency,\n",
        "            \"param_rows\": {\n",
        "                \"black\": ensure_param_columns(black_row, PARAM_COLS_BLACK),\n",
        "                \"heston\": ensure_param_columns(heston_row, PARAM_COLS_HESTON),\n",
        "                \"svcj\": ensure_param_columns(svcj_row, PARAM_COLS_SVCJ),\n",
        "            },\n",
        "            \"train_df\": empty_tt,\n",
        "            \"test_df\": empty_tt,\n",
        "            \"warm_next\": warm_next,\n",
        "        }\n",
        "\n",
        "    df_filt = filter_liquid_options(df_ccy, **filter_rules)\n",
        "\n",
        "    if df_filt is None or df_filt.empty or len(df_filt) < min_options_after_filter:\n",
        "        n_total = 0 if (df_filt is None) else int(len(df_filt))\n",
        "        msg = f\"Skipped: too few options after filtering (n={n_total}, min={min_options_after_filter}).\"\n",
        "        black_row = _make_param_row(\"black\", success=False, message=msg, nfev=0,\n",
        "                                   rmse_fit=np.nan, mae_fit=np.nan, rmse_train=np.nan, mae_train=np.nan,\n",
        "                                   rmse_test=np.nan, mae_test=np.nan,\n",
        "                                   n_total=n_total, n_train=0, n_test=0, params={\"sigma\": np.nan})\n",
        "        heston_row = _make_param_row(\"heston\", success=False, message=msg, nfev=0,\n",
        "                                    rmse_fit=np.nan, mae_fit=np.nan, rmse_train=np.nan, mae_train=np.nan,\n",
        "                                    rmse_test=np.nan, mae_test=np.nan,\n",
        "                                    n_total=n_total, n_train=0, n_test=0, params={})\n",
        "        svcj_row = _make_param_row(\"svcj\", success=False, message=msg, nfev=0,\n",
        "                                  rmse_fit=np.nan, mae_fit=np.nan, rmse_train=np.nan, mae_train=np.nan,\n",
        "                                  rmse_test=np.nan, mae_test=np.nan,\n",
        "                                  n_total=n_total, n_train=0, n_test=0, params={})\n",
        "        return {\n",
        "            \"timestamp_iso\": ts_iso,\n",
        "            \"currency\": currency,\n",
        "            \"param_rows\": {\n",
        "                \"black\": ensure_param_columns(black_row, PARAM_COLS_BLACK),\n",
        "                \"heston\": ensure_param_columns(heston_row, PARAM_COLS_HESTON),\n",
        "                \"svcj\": ensure_param_columns(svcj_row, PARAM_COLS_SVCJ),\n",
        "            },\n",
        "            \"train_df\": empty_tt,\n",
        "            \"test_df\": empty_tt,\n",
        "            \"warm_next\": warm_next,\n",
        "        }\n",
        "\n",
        "    # Optional runtime restriction\n",
        "    df_filt = restrict_for_runtime(\n",
        "        df_filt,\n",
        "        top_expiries=runtime_top_expiries_by_oi,\n",
        "        max_options=runtime_max_options,\n",
        "        random_state=random_seed,\n",
        "    )\n",
        "\n",
        "    # Precompute per-expiry FFTParams using *all* filtered options in this snapshot.\n",
        "    def _fft_params_for_expiry(strikes: np.ndarray) -> FFTParams:\n",
        "        N = fft_base.N\n",
        "        eta = fft_base.eta\n",
        "        lam = 2.0 * np.pi / (N * eta)\n",
        "        logK_center = float(np.log(np.median(strikes)))\n",
        "        b = logK_center - 0.5 * N * lam\n",
        "        return FFTParams(N=N, alpha=fft_base.alpha, eta=eta, b=b, use_simpson=fft_base.use_simpson)\n",
        "\n",
        "    fft_params_by_expiry: dict = {}\n",
        "    for exp, g in df_filt.groupby(\"expiry_datetime\", sort=False):\n",
        "        K_all = g[\"strike\"].to_numpy(dtype=float)\n",
        "        fft_params_by_expiry[exp] = _fft_params_for_expiry(K_all)\n",
        "\n",
        "    # Deterministic split (shuffled)\n",
        "    df_filt = df_filt.sample(frac=1.0, random_state=random_seed).reset_index(drop=True)\n",
        "    n_train = int(np.floor(train_frac * len(df_filt)))\n",
        "    train = df_filt.iloc[:n_train].copy()\n",
        "    test = df_filt.iloc[n_train:].copy()\n",
        "\n",
        "    # Warm-start initializations\n",
        "    init_black = warm_start.get(\"black\", None)\n",
        "\n",
        "    def _init_heston_from_black(sigma: float, prev: Optional[dict[str, float]]) -> dict[str, float]:\n",
        "        sigma2 = float(sigma) * float(sigma)\n",
        "        init = dict(prev) if prev else {}\n",
        "        init.setdefault(\"kappa\", 2.0)\n",
        "        init.setdefault(\"theta\", 0.5 * sigma2)\n",
        "        init.setdefault(\"sigma_v\", 0.75 * sigma)\n",
        "        init.setdefault(\"rho\", -0.5)\n",
        "        init.setdefault(\"v0\", 0.5 * sigma2)\n",
        "        return init\n",
        "\n",
        "    def _init_svcj_from_black(sigma: float, prev: Optional[dict[str, float]]) -> dict[str, float]:\n",
        "        sigma2 = float(sigma) * float(sigma)\n",
        "        init = dict(prev) if prev else {}\n",
        "        init.setdefault(\"kappa\", 2.0)\n",
        "        init.setdefault(\"theta\", 0.5 * sigma2)\n",
        "        init.setdefault(\"sigma_v\", 0.75 * sigma)\n",
        "        init.setdefault(\"rho\", -0.5)\n",
        "        init.setdefault(\"v0\", 0.5 * sigma2)\n",
        "        # Jump defaults (mild)\n",
        "        init.setdefault(\"lam\", 0.10)\n",
        "        init.setdefault(\"ell_y\", -0.05)\n",
        "        init.setdefault(\"sigma_y\", 0.15)\n",
        "        init.setdefault(\"ell_v\", 0.10)\n",
        "        init.setdefault(\"rho_j\", -0.3)\n",
        "        return init\n",
        "\n",
        "    models = [\"black\", \"heston\", \"svcj\"]\n",
        "    results: dict[str, Any] = {}\n",
        "    sigma_seed: Optional[float] = None\n",
        "\n",
        "    for model in models:\n",
        "        if verbose:\n",
        "            print(f\"[{currency}] {ts_iso} | {model}: calibrating on n_train={len(train)}\")\n",
        "\n",
        "        if model == \"black\":\n",
        "            init_params = init_black\n",
        "        elif model == \"heston\":\n",
        "            if sigma_seed is None:\n",
        "                if init_black and \"sigma\" in init_black:\n",
        "                    sigma_seed = float(init_black[\"sigma\"])\n",
        "                else:\n",
        "                    sigma_seed = 0.6\n",
        "            init_params = _init_heston_from_black(sigma_seed, warm_start.get(\"heston\", None))\n",
        "        else:\n",
        "            if sigma_seed is None:\n",
        "                if init_black and \"sigma\" in init_black:\n",
        "                    sigma_seed = float(init_black[\"sigma\"])\n",
        "                else:\n",
        "                    sigma_seed = 0.6\n",
        "            init_params = _init_svcj_from_black(sigma_seed, warm_start.get(\"svcj\", None))\n",
        "\n",
        "        try:\n",
        "            res = calibrate_model(\n",
        "                train,\n",
        "                model,\n",
        "                weight_config=weight_config,\n",
        "                fft_params_base=fft_base,\n",
        "                dynamic_b=False,\n",
        "                fft_params_by_expiry=fft_params_by_expiry,\n",
        "                use_cache_in_optimization=False,\n",
        "                initial_params=init_params,\n",
        "                max_nfev=int(max_nfev[model]),\n",
        "                verbose=1 if verbose else 0,\n",
        "                clear_cache_before=False,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            results[model] = {\"success\": False, \"message\": f\"Exception: {repr(e)}\", \"nfev\": 0, \"params\": {}}\n",
        "            continue\n",
        "\n",
        "        results[model] = res\n",
        "        if getattr(res, \"success\", False):\n",
        "            warm_next[model] = dict(res.params)\n",
        "            if model == \"black\":\n",
        "                try:\n",
        "                    sigma_seed = float(res.params.get(\"sigma\", np.nan))\n",
        "                except Exception:\n",
        "                    sigma_seed = sigma_seed\n",
        "\n",
        "    # Reprice full snapshot once per model (then split train/test for output & errors)\n",
        "    df_out = df_filt.copy()\n",
        "    df_out[\"snapshot_ts\"] = ts_iso\n",
        "    df_out[\"currency\"] = currency\n",
        "    df_out[\"random_seed\"] = int(random_seed)\n",
        "\n",
        "    price_cols = {\"black\": \"price_black\", \"heston\": \"price_heston\", \"svcj\": \"price_svcj\"}\n",
        "    for model in models:\n",
        "        col = price_cols[model]\n",
        "        if model in results and hasattr(results[model], \"params\") and getattr(results[model], \"success\", False):\n",
        "            try:\n",
        "                p = price_dataframe(\n",
        "                    df_out,\n",
        "                    model,\n",
        "                    dict(results[model].params),\n",
        "                    fft_params_base=fft_base,\n",
        "                    dynamic_b=False,\n",
        "                    fft_params_by_expiry=fft_params_by_expiry,\n",
        "                    use_cache=True,\n",
        "                )\n",
        "                df_out[col] = p\n",
        "            except Exception as e:\n",
        "                df_out[col] = np.nan\n",
        "                if verbose:\n",
        "                    print(f\"[{currency}] {ts_iso} | {model}: pricing exception: {repr(e)}\")\n",
        "        else:\n",
        "            df_out[col] = np.nan\n",
        "\n",
        "    train_out = df_out.iloc[:n_train].copy()\n",
        "    test_out = df_out.iloc[n_train:].copy()\n",
        "\n",
        "    # Errors (unweighted)\n",
        "    y_train = train_out[\"mid_price_clean\"].to_numpy(dtype=float) if len(train_out) else np.array([])\n",
        "    y_test = test_out[\"mid_price_clean\"].to_numpy(dtype=float) if len(test_out) else np.array([])\n",
        "\n",
        "    errs: dict[str, dict[str, float]] = {}\n",
        "    for model in models:\n",
        "        col = price_cols[model]\n",
        "        p_train = train_out[col].to_numpy(dtype=float) if len(train_out) else np.array([])\n",
        "        p_test = test_out[col].to_numpy(dtype=float) if len(test_out) else np.array([])\n",
        "        e_tr = compute_errors(y_train, p_train) if len(train_out) else {\"mse\": np.nan, \"mae\": np.nan}\n",
        "        e_te = compute_errors(y_test, p_test) if len(test_out) else {\"mse\": np.nan, \"mae\": np.nan}\n",
        "        errs[model] = dict(\n",
        "            rmse_train=float(math.sqrt(e_tr[\"mse\"])) if np.isfinite(e_tr[\"mse\"]) else float(\"nan\"),\n",
        "            mae_train=float(e_tr[\"mae\"]),\n",
        "            rmse_test=float(math.sqrt(e_te[\"mse\"])) if np.isfinite(e_te[\"mse\"]) else float(\"nan\"),\n",
        "            mae_test=float(e_te[\"mae\"]),\n",
        "        )\n",
        "\n",
        "    n_total = int(len(df_out))\n",
        "    n_test = int(len(test_out))\n",
        "\n",
        "    # Parameter rows\n",
        "    if isinstance(results.get(\"black\"), dict):\n",
        "        msg = results[\"black\"].get(\"message\", \"failed\")\n",
        "        black_row = _make_param_row(\"black\", success=False, message=msg, nfev=results[\"black\"].get(\"nfev\", 0),\n",
        "                                    rmse_fit=np.nan, mae_fit=np.nan,\n",
        "                                    rmse_train=errs[\"black\"][\"rmse_train\"], mae_train=errs[\"black\"][\"mae_train\"],\n",
        "                                    rmse_test=errs[\"black\"][\"rmse_test\"], mae_test=errs[\"black\"][\"mae_test\"],\n",
        "                                    n_total=n_total, n_train=n_train, n_test=n_test,\n",
        "                                    params={\"sigma\": np.nan})\n",
        "    else:\n",
        "        resb = results.get(\"black\", None)\n",
        "        if resb is None:\n",
        "            black_row = _make_param_row(\"black\", success=False, message=\"failed\", nfev=0,\n",
        "                                        rmse_fit=np.nan, mae_fit=np.nan,\n",
        "                                        rmse_train=errs[\"black\"][\"rmse_train\"], mae_train=errs[\"black\"][\"mae_train\"],\n",
        "                                        rmse_test=errs[\"black\"][\"rmse_test\"], mae_test=errs[\"black\"][\"mae_test\"],\n",
        "                                        n_total=n_total, n_train=n_train, n_test=n_test,\n",
        "                                        params={\"sigma\": np.nan})\n",
        "        else:\n",
        "            black_row = _make_param_row(\"black\", success=bool(resb.success), message=resb.message, nfev=resb.nfev,\n",
        "                                        rmse_fit=resb.rmse, mae_fit=resb.mae,\n",
        "                                        rmse_train=errs[\"black\"][\"rmse_train\"], mae_train=errs[\"black\"][\"mae_train\"],\n",
        "                                        rmse_test=errs[\"black\"][\"rmse_test\"], mae_test=errs[\"black\"][\"mae_test\"],\n",
        "                                        n_total=n_total, n_train=n_train, n_test=n_test,\n",
        "                                        params=dict(resb.params))\n",
        "\n",
        "    resh = results.get(\"heston\", None)\n",
        "    if isinstance(resh, dict):\n",
        "        heston_row = _make_param_row(\"heston\", success=False, message=resh.get(\"message\", \"failed\"), nfev=resh.get(\"nfev\", 0),\n",
        "                                     rmse_fit=np.nan, mae_fit=np.nan,\n",
        "                                     rmse_train=errs[\"heston\"][\"rmse_train\"], mae_train=errs[\"heston\"][\"mae_train\"],\n",
        "                                     rmse_test=errs[\"heston\"][\"rmse_test\"], mae_test=errs[\"heston\"][\"mae_test\"],\n",
        "                                     n_total=n_total, n_train=n_train, n_test=n_test, params={})\n",
        "    elif resh is None:\n",
        "        heston_row = _make_param_row(\"heston\", success=False, message=\"failed\", nfev=0,\n",
        "                                     rmse_fit=np.nan, mae_fit=np.nan,\n",
        "                                     rmse_train=errs[\"heston\"][\"rmse_train\"], mae_train=errs[\"heston\"][\"mae_train\"],\n",
        "                                     rmse_test=errs[\"heston\"][\"rmse_test\"], mae_test=errs[\"heston\"][\"mae_test\"],\n",
        "                                     n_total=n_total, n_train=n_train, n_test=n_test, params={})\n",
        "    else:\n",
        "        heston_row = _make_param_row(\"heston\", success=bool(resh.success), message=resh.message, nfev=resh.nfev,\n",
        "                                     rmse_fit=resh.rmse, mae_fit=resh.mae,\n",
        "                                     rmse_train=errs[\"heston\"][\"rmse_train\"], mae_train=errs[\"heston\"][\"mae_train\"],\n",
        "                                     rmse_test=errs[\"heston\"][\"rmse_test\"], mae_test=errs[\"heston\"][\"mae_test\"],\n",
        "                                     n_total=n_total, n_train=n_train, n_test=n_test, params=dict(resh.params))\n",
        "\n",
        "    ress = results.get(\"svcj\", None)\n",
        "    if isinstance(ress, dict):\n",
        "        svcj_row = _make_param_row(\"svcj\", success=False, message=ress.get(\"message\", \"failed\"), nfev=ress.get(\"nfev\", 0),\n",
        "                                   rmse_fit=np.nan, mae_fit=np.nan,\n",
        "                                   rmse_train=errs[\"svcj\"][\"rmse_train\"], mae_train=errs[\"svcj\"][\"mae_train\"],\n",
        "                                   rmse_test=errs[\"svcj\"][\"rmse_test\"], mae_test=errs[\"svcj\"][\"mae_test\"],\n",
        "                                   n_total=n_total, n_train=n_train, n_test=n_test, params={})\n",
        "    elif ress is None:\n",
        "        svcj_row = _make_param_row(\"svcj\", success=False, message=\"failed\", nfev=0,\n",
        "                                   rmse_fit=np.nan, mae_fit=np.nan,\n",
        "                                   rmse_train=errs[\"svcj\"][\"rmse_train\"], mae_train=errs[\"svcj\"][\"mae_train\"],\n",
        "                                   rmse_test=errs[\"svcj\"][\"rmse_test\"], mae_test=errs[\"svcj\"][\"mae_test\"],\n",
        "                                   n_total=n_total, n_train=n_train, n_test=n_test, params={})\n",
        "    else:\n",
        "        svcj_row = _make_param_row(\"svcj\", success=bool(ress.success), message=ress.message, nfev=ress.nfev,\n",
        "                                   rmse_fit=ress.rmse, mae_fit=ress.mae,\n",
        "                                   rmse_train=errs[\"svcj\"][\"rmse_train\"], mae_train=errs[\"svcj\"][\"mae_train\"],\n",
        "                                   rmse_test=errs[\"svcj\"][\"rmse_test\"], mae_test=errs[\"svcj\"][\"mae_test\"],\n",
        "                                   n_total=n_total, n_train=n_train, n_test=n_test, params=dict(ress.params))\n",
        "\n",
        "    black_row = ensure_param_columns(black_row, PARAM_COLS_BLACK)\n",
        "    heston_row = ensure_param_columns(heston_row, PARAM_COLS_HESTON)\n",
        "    svcj_row = ensure_param_columns(svcj_row, PARAM_COLS_SVCJ)\n",
        "\n",
        "    train_out[\"snapshot_ts\"] = normalize_timestamp_series(train_out[\"snapshot_ts\"])\n",
        "    test_out[\"snapshot_ts\"] = normalize_timestamp_series(test_out[\"snapshot_ts\"])\n",
        "\n",
        "    return {\n",
        "        \"timestamp_iso\": ts_iso,\n",
        "        \"currency\": currency,\n",
        "        \"param_rows\": {\"black\": black_row, \"heston\": heston_row, \"svcj\": svcj_row},\n",
        "        \"train_df\": train_out,\n",
        "        \"test_df\": test_out,\n",
        "        \"warm_next\": warm_next,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0401400b",
      "metadata": {},
      "source": [
        "## Runner: loop all snapshots for one currency (resume + threading + ordered commits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a25fdb50",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_all_snapshots_to_excel_for_currency(\n",
        "    currency: str,\n",
        "    *,\n",
        "    output_xlsx: Path,\n",
        "    resume: bool,\n",
        "    verbose: bool = False,\n",
        ") -> dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Runs calibration for one currency and persists to Excel every SAVE_EVERY_N_FILES commits.\n",
        "\n",
        "    Returns the in-memory workbook dict (DataFrames) after completion.\n",
        "    \"\"\"\n",
        "    data_dir = ROOT / \"data\"\n",
        "    all_files = list_snapshot_files(data_dir)\n",
        "    if not all_files:\n",
        "        raise RuntimeError(f\"No snapshot files found in {data_dir}\")\n",
        "\n",
        "    all_files_sorted = sorted(all_files, key=timestamp_from_filename)\n",
        "    file_index_map = {p: i for i, p in enumerate(all_files_sorted)}\n",
        "    currency_index = CURRENCIES.index(currency) if currency in CURRENCIES else 0\n",
        "\n",
        "    wb = load_existing_workbook(output_xlsx)\n",
        "    last_ts = get_latest_processed_timestamp(wb, currency) if resume else None\n",
        "\n",
        "    pending = []\n",
        "    for f in all_files_sorted:\n",
        "        ts = timestamp_from_filename(f)\n",
        "        if last_ts is None or ts > last_ts:\n",
        "            pending.append(f)\n",
        "\n",
        "    if SMOKE_TEST_MAX_FILES_PER_CURRENCY is not None:\n",
        "        pending = pending[: int(SMOKE_TEST_MAX_FILES_PER_CURRENCY)]\n",
        "\n",
        "    if not pending:\n",
        "        print(f\"[{currency}] Nothing to do. (resume={resume}, last_ts={last_ts})\")\n",
        "        return wb\n",
        "\n",
        "    print(f\"[{currency}] Pending files: {len(pending)} (resume={resume}, last_ts={last_ts})\")\n",
        "    print(f\"[{currency}] Output: {output_xlsx}\")\n",
        "\n",
        "    chunks = chunk_contiguous(pending, N_THREADS)\n",
        "\n",
        "    warm_init_by_chunk: list[dict[str, dict[str, float]]] = []\n",
        "    for ch in chunks:\n",
        "        if not ch:\n",
        "            warm_init_by_chunk.append({})\n",
        "            continue\n",
        "        _, f0 = ch[0]\n",
        "        ts0 = timestamp_from_filename(f0)\n",
        "\n",
        "        black_hist = latest_successful_params_before(\n",
        "            wb.get(PARAM_SHEET_BLACK, pd.DataFrame()),\n",
        "            currency=currency,\n",
        "            ts0=ts0,\n",
        "            required_cols=[\"sigma\"],\n",
        "        )\n",
        "        heston_hist = latest_successful_params_before(\n",
        "            wb.get(PARAM_SHEET_HESTON, pd.DataFrame()),\n",
        "            currency=currency,\n",
        "            ts0=ts0,\n",
        "            required_cols=[\"kappa\", \"theta\", \"sigma_v\", \"rho\", \"v0\"],\n",
        "        )\n",
        "        svcj_hist = latest_successful_params_before(\n",
        "            wb.get(PARAM_SHEET_SVCJ, pd.DataFrame()),\n",
        "            currency=currency,\n",
        "            ts0=ts0,\n",
        "            required_cols=[\"kappa\", \"theta\", \"sigma_v\", \"rho\", \"v0\", \"lam\", \"ell_y\", \"sigma_y\", \"ell_v\", \"rho_j\"],\n",
        "        )\n",
        "        warm_init: dict[str, dict[str, float]] = {}\n",
        "        if black_hist:\n",
        "            warm_init[\"black\"] = black_hist\n",
        "        if heston_hist:\n",
        "            warm_init[\"heston\"] = heston_hist\n",
        "        if svcj_hist:\n",
        "            warm_init[\"svcj\"] = svcj_hist\n",
        "        warm_init_by_chunk.append(warm_init)\n",
        "\n",
        "    q: queue.Queue = queue.Queue()\n",
        "    n_workers = len(chunks)\n",
        "\n",
        "    def _worker(chunk_id: int, chunk: list[tuple[int, Path]], warm0: dict[str, dict[str, float]]) -> None:\n",
        "        warm = {k: dict(v) for k, v in (warm0 or {}).items()}\n",
        "        cm = threadpool_limits(limits=INTERNAL_NUM_THREADS) if (LIMIT_INTERNAL_THREADS and threadpool_limits is not None) else contextlib.nullcontext()\n",
        "        with cm:\n",
        "            for idx, path in chunk:\n",
        "                seed = int(GLOBAL_RANDOM_SEED + 10_000 * currency_index + file_index_map[path])\n",
        "                try:\n",
        "                    payload = process_snapshot_to_excel_payload(\n",
        "                        path,\n",
        "                        currency=currency,\n",
        "                        filter_rules=FILTER_RULES,\n",
        "                        weight_config=WEIGHT_CONFIG,\n",
        "                        fft_base=FFT_BASE,\n",
        "                        max_nfev=MAX_NFEV,\n",
        "                        train_frac=TRAIN_FRAC,\n",
        "                        random_seed=seed,\n",
        "                        runtime_top_expiries_by_oi=RUNTIME_TOP_EXPIRIES_BY_OI,\n",
        "                        runtime_max_options=RUNTIME_MAX_OPTIONS,\n",
        "                        min_options_after_filter=MIN_OPTIONS_AFTER_FILTER,\n",
        "                        warm_start=warm,\n",
        "                        verbose=verbose,\n",
        "                    )\n",
        "                    warm = payload.get(\"warm_next\", warm)\n",
        "                except Exception as e:\n",
        "                    # Catastrophic failure; still advance resume key (Option A)\n",
        "                    ts_iso = timestamp_to_iso_z(timestamp_from_filename(path))\n",
        "                    msg = f\"Worker exception: {repr(e)}\"\n",
        "                    black_row = pd.DataFrame([{\n",
        "                        \"timestamp\": ts_iso,\n",
        "                        \"currency\": currency,\n",
        "                        \"success\": False,\n",
        "                        \"message\": msg,\n",
        "                        \"nfev\": 0,\n",
        "                        \"rmse_fit\": np.nan, \"mae_fit\": np.nan,\n",
        "                        \"rmse_train\": np.nan, \"mae_train\": np.nan,\n",
        "                        \"rmse_test\": np.nan, \"mae_test\": np.nan,\n",
        "                        \"n_options_total\": 0, \"n_train\": 0, \"n_test\": 0,\n",
        "                        \"random_seed\": seed,\n",
        "                        \"sigma\": np.nan,\n",
        "                    }])\n",
        "                    payload = {\n",
        "                        \"timestamp_iso\": ts_iso,\n",
        "                        \"currency\": currency,\n",
        "                        \"param_rows\": {\n",
        "                            \"black\": ensure_param_columns(black_row, PARAM_COLS_BLACK),\n",
        "                            \"heston\": ensure_param_columns(pd.DataFrame([{\n",
        "                                \"timestamp\": ts_iso, \"currency\": currency, \"success\": False, \"message\": msg, \"nfev\": 0,\n",
        "                                \"rmse_fit\": np.nan, \"mae_fit\": np.nan, \"rmse_train\": np.nan, \"mae_train\": np.nan,\n",
        "                                \"rmse_test\": np.nan, \"mae_test\": np.nan, \"n_options_total\": 0, \"n_train\": 0, \"n_test\": 0,\n",
        "                                \"random_seed\": seed\n",
        "                            }]), PARAM_COLS_HESTON),\n",
        "                            \"svcj\": ensure_param_columns(pd.DataFrame([{\n",
        "                                \"timestamp\": ts_iso, \"currency\": currency, \"success\": False, \"message\": msg, \"nfev\": 0,\n",
        "                                \"rmse_fit\": np.nan, \"mae_fit\": np.nan, \"rmse_train\": np.nan, \"mae_train\": np.nan,\n",
        "                                \"rmse_test\": np.nan, \"mae_test\": np.nan, \"n_options_total\": 0, \"n_train\": 0, \"n_test\": 0,\n",
        "                                \"random_seed\": seed\n",
        "                            }]), PARAM_COLS_SVCJ),\n",
        "                        },\n",
        "                        \"train_df\": pd.DataFrame(),\n",
        "                        \"test_df\": pd.DataFrame(),\n",
        "                        \"warm_next\": warm,\n",
        "                    }\n",
        "\n",
        "                q.put((idx, payload))\n",
        "\n",
        "        q.put((\"DONE\", chunk_id))\n",
        "\n",
        "    threads = []\n",
        "    for chunk_id, (chunk, warm0) in enumerate(zip(chunks, warm_init_by_chunk)):\n",
        "        t = threading.Thread(target=_worker, args=(chunk_id, chunk, warm0), daemon=True)\n",
        "        t.start()\n",
        "        threads.append(t)\n",
        "\n",
        "    pending_results: dict[int, dict[str, Any]] = {}\n",
        "    next_commit = 0\n",
        "    committed_since_flush = 0\n",
        "    done_count = 0\n",
        "    total_pending = len(pending)\n",
        "\n",
        "    def _commit_payload(payload: dict[str, Any]) -> None:\n",
        "        nonlocal wb\n",
        "        append_df(wb, PARAM_SHEET_BLACK, payload[\"param_rows\"][\"black\"])\n",
        "        append_df(wb, PARAM_SHEET_HESTON, payload[\"param_rows\"][\"heston\"])\n",
        "        append_df(wb, PARAM_SHEET_SVCJ, payload[\"param_rows\"][\"svcj\"])\n",
        "        append_df(wb, TRAIN_SHEET, payload.get(\"train_df\", pd.DataFrame()))\n",
        "        append_df(wb, TEST_SHEET, payload.get(\"test_df\", pd.DataFrame()))\n",
        "\n",
        "    while done_count < n_workers:\n",
        "        msg = q.get()\n",
        "        if isinstance(msg[0], str) and msg[0] == \"DONE\":\n",
        "            done_count += 1\n",
        "            continue\n",
        "\n",
        "        idx, payload = msg\n",
        "        pending_results[int(idx)] = payload\n",
        "\n",
        "        while next_commit in pending_results:\n",
        "            payload2 = pending_results.pop(next_commit)\n",
        "            _commit_payload(payload2)\n",
        "            committed_since_flush += 1\n",
        "            next_commit += 1\n",
        "\n",
        "            last_ts_iso = payload2.get(\"timestamp_iso\", \"?\")\n",
        "            print(f\"[{currency}] committed {next_commit}/{total_pending} (last={last_ts_iso})\")\n",
        "\n",
        "            if committed_since_flush >= SAVE_EVERY_N_FILES:\n",
        "                flush_workbook_atomic(wb, output_xlsx)\n",
        "                committed_since_flush = 0\n",
        "                print(f\"[{currency}] flushed → {output_xlsx}\")\n",
        "\n",
        "    for t in threads:\n",
        "        t.join(timeout=0.1)\n",
        "\n",
        "    flush_workbook_atomic(wb, output_xlsx)\n",
        "    print(f\"[{currency}] final flush → {output_xlsx}\")\n",
        "\n",
        "    return wb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0a8052b",
      "metadata": {},
      "source": [
        "## Run everything: BTC then ETH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbf8ec87",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run calibrations and persist to Excel\n",
        "wb_final = None\n",
        "for ccy in CURRENCIES:\n",
        "    wb_final = run_all_snapshots_to_excel_for_currency(\n",
        "        ccy,\n",
        "        output_xlsx=OUTPUT_XLSX,\n",
        "        resume=RESUME,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "print(\"Done. Workbook at:\", OUTPUT_XLSX)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
